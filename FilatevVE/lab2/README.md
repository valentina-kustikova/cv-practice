## Практическая работа №2. Детектирование объектов на изображениях с использованием библиотеки OpenCV

**Задача.** Разработать приложение для детектирования транспортных средств с использованием обученных
нейронных сетей, которые доступны среди "зоопарка" моделей OpenCV. При выборе моделей необходимо
обратить внимание на наличие интересующих классов транспортных средств среди детектируемых классов
объектов. Архив с кадрами видео доступен по [ссылке](https://cloud.unn.ru/s/nLkk7BXBqapNgcE), разметка
в текстовом формате доступна по [ссылке](https://cloud.unn.ru/s/j4wA4nx8mZ4yfqD).
---

## Структура проекта

```
lab2/
├── lib/                  # Библиотека с реализацией детекторов
│   ├── base_detector.py  # Базовый абстрактный класс для всех детекторов
│   ├── nanodet.py        # Реализация детектора NanoDet
│   ├── yolox.py          # Реализация детектора YoloX
│   ├── ssd.py            # Реализация детектора SSD
│   ├── common.py         # Вспомогательные функции (загрузка файлов, визуализация)
│   └── settings.py       # Конфигурация моделей и список классов COCO
├── main.py               # Главное приложение для работы с детекторами
├── requirements.txt      # Зависимости проекта
├── models/               # Папка с ONNX моделями
│   ├── nanodet.onnx
│   ├── yolox.onnx
│   └── ssd.onnx
├── images/               # Папка с тестовыми изображениями
│   ├── img1.jpg
│   ├── img2.jpg
│   ├── img3.jpg
│   └── ...
├── videos/               # Папка с тестовыми видео
│   ├── vidio1.mp4
│   ├── vidio2.mp4
│   └── video3.mp4
└── README.md             # Документация проекта
```

## Использование

1. Установите необходимые зависимости:
   ```bash
   pip install -r requirements.txt
   ```

2. Запустите приложение:
   ```bash
   python main.py
   ```

3. Выберите тип источника (изображение или видео)

4. Выберите модель детектирования:
   - **NanoDet** - быстрая модель для детектирования объектов
   - **YoloX** - точная модель для детектирования объектов
   - **SSD** - сбалансированная модель для детектирования объектов

5. При работе с изображениями используйте клавиши:
   - **D** - следующее изображение
   - **A** - предыдущее изображение
   - **ESC** - выход

6. При работе с видео используйте клавишу **ESC** для выхода

**Примечание:** Модели автоматически скачиваются при первом запуске, если их нет в папке `models/`.

---

## Описание реализованных моделей

### Архитектура классов

Проект использует иерархию классов для детектирования объектов:
- `BaseDetector` - абстрактный базовый класс, определяющий интерфейс для всех детекторов
- `NanoDet`, `YoloX`, `SSD` - конкретные реализации детекторов, наследующиеся от `BaseDetector`

Каждый детектор реализует методы:
- `preprocess()` - предобработка изображения перед подачей в сеть
- `postprocess()` - постобработка выходов сети для получения детекций
- `infer()` - выполнение инференса на изображении

---

### 1. NanoDet

**Описание модели:** NanoDet - это легковесная модель детектирования объектов, оптимизированная для мобильных устройств и быстрой работы.

#### Предобработка изображения:

1. **Letterbox преобразование:**
   - Изображение масштабируется с сохранением пропорций до размера 416×416
   - Если изображение не квадратное, добавляются черные границы (padding) для получения квадратного формата
   - Вычисляется масштаб преобразования для последующего восстановления координат

2. **Нормализация:**
   - Изображение преобразуется в формат float32
   - Применяется нормализация по формуле: `(pixel - mean) / std`
   - Значения mean: `[103.53, 116.28, 123.675]`
   - Значения std: `[57.375, 57.12, 58.395]`

3. **Создание blob:**
   - Изображение преобразуется в формат blob через `cv2.dnn.blobFromImage()`
   - Формат: (1, 3, 416, 416)

#### Постобработка выхода сети:

1. **Обработка выходов сети:**
   - Модель возвращает выходы для 4 уровней признаков (strides: 8, 16, 32, 64)
   - Для каждого уровня получаются:
     - `cls_scores` - оценки классов
     - `bbox_preds` - предсказания координат в формате распределения (distribution)

2. **Декодирование координат:**
   - Предсказания координат представлены в виде распределения по `reg_max + 1` значениям (reg_max = 7)
   - Применяется softmax для нормализации распределения
   - Координаты восстанавливаются через скалярное произведение с проекционным вектором
   - Координаты умножаются на соответствующий stride уровня

3. **Преобразование в формат (x1, y1, x2, y2):**
   - Центры якорей (anchors) вычисляются заранее для каждого уровня
   - Координаты боксов вычисляются как:
     - `x1 = center_x - distance_left`
     - `y1 = center_y - distance_top`
     - `x2 = center_x + distance_right`
     - `y2 = center_y + distance_bottom`
   - Координаты обрезаются границами изображения

4. **Фильтрация и NMS:**
   - Выбираются топ-k детекций по максимальной уверенности (nms_pre = 1000)
   - Определяется класс с максимальной уверенностью для каждой детекции
   - Применяется Non-Maximum Suppression (NMS) с порогами:
     - `probThreshold = 0.35` - минимальная уверенность
     - `iouThreshold = 0.6` - порог пересечения для NMS

5. **Формат результата:**
   - Массив детекций в формате: `[x1, y1, x2, y2, confidence, class_id]`

---

### 2. YoloX

**Описание модели:** YoloX - это улучшенная версия YOLO, обеспечивающая высокую точность детектирования объектов.

#### Предобработка изображения:

1. **Letterbox преобразование:**
   - Изображение масштабируется с сохранением пропорций до размера 640×640
   - Используется padding серым цветом (значение 114.0) для получения квадратного формата
   - Вычисляется коэффициент масштабирования (ratio) для последующего восстановления координат

2. **Преобразование формата:**
   - Изображение преобразуется из формата (H, W, 3) в формат (1, 3, H, W)
   - Используется транспонирование: `np.transpose(img, (2, 0, 1))`
   - Добавляется размерность батча: `blob[np.newaxis, :, :, :]`

**Примечание:** YoloX не требует нормализации mean/std, так как модель обучена на изображениях в диапазоне [0, 1] после letterbox преобразования.

#### Постобработка выхода сети:

1. **Обработка выходов сети:**
   - Модель возвращает один тензор с детекциями
   - Формат: `[center_x, center_y, width, height, obj_score, class_scores...]`
   - Используются 3 уровня признаков (strides: 8, 16, 32)

2. **Генерация якорей (anchors):**
   - Якори генерируются заранее для каждого уровня признаков
   - Для каждого уровня создается сетка координат (grid)
   - Вычисляются expanded_strides для каждого уровня

3. **Декодирование координат:**
   - Предсказанные координаты декодируются с использованием якорей:
     - `center_x = (pred_x + grid_x) * stride`
     - `center_y = (pred_y + grid_y) * stride`
     - `width = exp(pred_w) * stride`
     - `height = exp(pred_h) * stride`

4. **Преобразование в формат (x1, y1, x2, y2):**
   - Координаты преобразуются из формата (center_x, center_y, width, height):
     - `x1 = center_x - width / 2`
     - `y1 = center_y - height / 2`
     - `x2 = center_x + width / 2`
     - `y2 = center_y + height / 2`

5. **Вычисление уверенности:**
   - Итоговая уверенность = `obj_score * class_score`
   - Выбирается класс с максимальной уверенностью

6. **Фильтрация и NMS:**
   - Применяется батч-версия NMS: `cv2.dnn.NMSBoxesBatched()`
   - Пороги:
     - `confThreshold = 0.5` - минимальная уверенность
     - `nmsThreshold = 0.5` - порог пересечения для NMS
     - `objThreshold = 0.5` - порог для объектности

7. **Формат результата:**
   - Массив детекций в формате: `[x1, y1, x2, y2, confidence, class_id]`

---

### 3. SSD (Single Shot Detector)

**Описание модели:** SSD MobileNet - это эффективная модель детектирования объектов, основанная на архитектуре MobileNet.

#### Предобработка изображения:

1. **Изменение размера:**
   - Изображение масштабируется до размера 300×300 пикселей
   - Используется билинейная интерполяция (`cv2.INTER_LINEAR`)

2. **Нормализация:**
   - При использовании OpenCV DNN:
     - Применяется нормализация: `(pixel - 127.5) * 0.007843`
     - Эквивалентно: `(pixel - 127.5) / 127.5`
   - При использовании ONNX Runtime:
     - Изображение остается в формате uint8 без нормализации

3. **Создание blob:**
   - При использовании OpenCV DNN:
     - Используется `cv2.dnn.blobFromImage()` с параметрами:
       - `scalefactor = 0.007843`
       - `mean = (127.5, 127.5, 127.5)`
       - `swapRB = False` (изображение уже в RGB)
   - При использовании ONNX Runtime:
     - Изображение преобразуется в формат (1, H, W, 3) в формате uint8

**Примечание:** Модель может использовать как OpenCV DNN, так и ONNX Runtime в зависимости от совместимости модели.

#### Постобработка выхода сети:

1. **Обработка выходов сети:**
   - Модель возвращает 4 выхода:
     - `boxes` - координаты боксов в формате (1, 100, 4) как [y1, x1, y2, x2] (нормализованные 0-1)
     - `classes` - идентификаторы классов (1, 100)
     - `scores` - уверенности детекций (1, 100)
     - `num_detections` - количество валидных детекций (1,)

2. **Денормализация координат:**
   - Координаты преобразуются из нормализованного формата [0, 1] в пиксели:
     - `x1 = boxes[i, 1] * 300`
     - `y1 = boxes[i, 0] * 300`
     - `x2 = boxes[i, 3] * 300`
     - `y2 = boxes[i, 2] * 300`

3. **Коррекция индексов классов:**
   - Классы в модели проиндексированы начиная с 1 (0 - фон)
   - Индексы уменьшаются на 1 для соответствия стандартному формату (0-79)

4. **Фильтрация по уверенности:**
   - Отбрасываются детекции с уверенностью ниже `confThreshold = 0.5`
   - Отбрасываются детекции класса 0 (фон)

5. **Нормализация координат:**
   - Обеспечивается правильный порядок координат: `min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)`

6. **Non-Maximum Suppression:**
   - Применяется NMS с порогами:
     - `confThreshold = 0.5` - минимальная уверенность
     - `nmsThreshold = 0.4` - порог пересечения для NMS
   - Координаты преобразуются в формат (x, y, width, height) для NMS

7. **Формат результата:**
   - Массив детекций в формате: `[x1, y1, x2, y2, confidence, class_id]`

---

## Особенности реализации

### Иерархия классов

Все детекторы наследуются от базового класса `BaseDetector`, который определяет:
- Общий интерфейс для загрузки моделей через OpenCV DNN
- Абстрактные методы `preprocess()` и `postprocess()`
- Метод `infer()` для выполнения инференса

### Восстановление координат

Для всех моделей используется функция `unletterbox()` для преобразования координат из пространства модели обратно в пространство исходного изображения с учетом letterbox преобразования.

### Визуализация

Функция `vis()` отображает детекции на изображении:
- Рисует прямоугольники вокруг обнаруженных объектов
- Отображает название класса и уверенность в левом верхнем углу прямоугольника
- Форматирует уверенность с тремя знаками после запятой

---

## Детектируемые классы

Все модели детектируют объекты из набора COCO (80 классов), включая транспортные средства:
- `car` (автомобиль)
- `truck` (грузовик)
- `bus` (автобус)
- `motorcycle` (мотоцикл)
- `bicycle` (велосипед)

Полный список классов доступен в файле `lib/settings.py`.
