# Практическая работа №3. Классификация изображений с использованием библиотеки OpenCV

## Описание

Данное приложение предназначено для классификации изображений известных достопримечательностей Нижнего Новгорода:
- Нижегородский Кремль
- Архангельский собор
- Дворец труда

Приложение реализует два алгоритма классификации:
1. **Алгоритм "мешок слов" (Bag of Words)**
2. **Нейросетевой классификатор с transfer learning**

## Структура проекта

```
.
├── main.py                 # Главный скрипт приложения (общий)
├── run_bow.py             # Скрипт для запуска только Bag of Words
├── run_nn.py              # Скрипт для запуска только нейронной сети
├── compare_detectors.py   # Скрипт для сравнения детекторов BoW
├── research_vocab_sizes.py # Скрипт для исследования размеров словаря
├── data_loader.py         # Модуль загрузки данных
├── bag_of_words.py        # Реализация алгоритма Bag of Words
├── neural_network.py      # Реализация нейросетевого классификатора
├── requirements.txt       # Зависимости проекта
├── README.md             # Документация
└── train.txt             # Файл со списком тренировочных изображений
```

## Установка зависимостей

```bash
pip install -r requirements.txt
```

## Использование

### Быстрый старт

**Запуск Bag of Words:**
```bash
python run_bow.py --data_dir . --train_file train.txt
```

**Запуск нейронной сети:**
```bash
python run_nn.py --data_dir . --train_file train.txt
```

**Сравнение детекторов:**
```bash
python compare_detectors.py --data_dir . --train_file train.txt
```

### Базовое использование (через main.py)

```bash
python main.py --data_dir <путь_к_данным> --train_file train.txt
```

### Параметры командной строки

#### Обязательные параметры:
- `--data_dir` - Путь до директории с данными
- `--train_file` - Путь к файлу со списком тренировочных изображений

#### Опциональные параметры:

**Режим работы:**
- `--mode` - Режим работы: `train` (обучение), `test` (тестирование), `both` (обучение и тестирование). По умолчанию: `both`

**Алгоритм:**
- `--algorithm` - Алгоритм: `bow` (мешок слов), `nn` (нейронная сеть), `both` (оба алгоритма). По умолчанию: `both`

**Параметры Bag of Words:**
- `--bow_vocab_size` - Размер словаря (количество кластеров). По умолчанию: 100
- `--bow_detector` - Тип детектора: `SIFT`, `ORB`, `AKAZE`. По умолчанию: `SIFT`
- `--bow_model_path` - Путь для сохранения/загрузки модели. По умолчанию: `bow_model.pkl`

**Параметры нейронной сети:**
- `--nn_base_model` - Базовая модель: `ResNet50`, `VGG16`, `MobileNetV2`. По умолчанию: `ResNet50`
- `--nn_epochs` - Количество эпох обучения. По умолчанию: 20
- `--nn_batch_size` - Размер батча. По умолчанию: 16
- `--nn_learning_rate` - Скорость обучения. По умолчанию: 0.0001
- `--nn_model_path` - Путь для сохранения/загрузки модели. По умолчанию: `nn_model.h5`

### Примеры использования

**Обучение и тестирование обоих алгоритмов:**
```bash
python main.py --data_dir . --train_file train.txt --mode both --algorithm both
```

**Только обучение Bag of Words:**
```bash
python main.py --data_dir . --train_file train.txt --mode train --algorithm bow --bow_vocab_size 150 --bow_detector SIFT
```

**Только тестирование нейронной сети:**
```bash
python main.py --data_dir . --train_file train.txt --mode test --algorithm nn --nn_model_path nn_model.h5
```

**Обучение нейронной сети с кастомными параметрами:**
```bash
python main.py --data_dir . --train_file train.txt --mode train --algorithm nn --nn_base_model VGG16 --nn_epochs 30 --nn_batch_size 8
```

**Использование отдельных скриптов:**

*Запуск только Bag of Words с визуализацией:*
```bash
python run_bow.py --data_dir . --train_file train.txt --mode both --visualize
```

*Запуск нейронной сети с ResNet50:*
```bash
python run_nn.py --data_dir . --train_file train.txt --mode both --base_model ResNet50 --epochs 30
```

*Сравнение всех доступных детекторов:*
```bash
python compare_detectors.py --data_dir . --train_file train.txt --vocab_size 200
```

*Исследование влияния размера словаря:*
```bash
python research_vocab_sizes.py --data_dir . --train_file train.txt --detector SIFT --vocab_sizes 100 150 200 250 300
```

## Описание реализованных алгоритмов

### 1. Алгоритм "мешок слов" (Bag of Words)

#### Принцип работы:

Алгоритм Bag of Words для классификации изображений состоит из следующих этапов:

1. **Извлечение признаков:**
   - Используются детекторы ключевых точек (SIFT, ORB, AKAZE)
   - Для каждой ключевой точки вычисляется дескриптор
   - Дескрипторы описывают локальные особенности изображения

2. **Построение словаря визуальных слов:**
   - Все дескрипторы из тренировочных изображений собираются вместе
   - Применяется алгоритм K-means кластеризации для группировки похожих дескрипторов
   - Каждый кластер представляет собой "визуальное слово"
   - Размер словаря (количество кластеров) является параметром алгоритма

3. **Преобразование изображений в гистограммы:**
   - Для каждого изображения извлекаются дескрипторы
   - Каждый дескриптор сопоставляется с ближайшим визуальным словом
   - Строится гистограмма частоты встречаемости визуальных слов
   - Гистограмма нормализуется (сумма равна 1)

4. **Классификация:**
   - Гистограммы используются как признаки для обучения классификатора
   - Используется метод опорных векторов (SVM) с RBF ядром
   - Классификатор обучается на тренировочных данных

#### Поддерживаемые детекторы и дескрипторы:
- **SIFT** (Scale-Invariant Feature Transform) - наиболее точный, но медленный
- **ORB** (Oriented FAST and Rotated BRIEF) - быстрый, подходит для реального времени
- **AKAZE** (Accelerated-KAZE) - баланс между скоростью и точностью

Для сравнения различных детекторов можно использовать скрипт `compare_detectors.py`.

#### Визуализация:
Реализована визуализация ключевых точек для анализа качества детектирования признаков:
```bash
python run_bow.py --visualize
```

#### Преимущества:
- Не требует больших вычислительных ресурсов
- Хорошо работает на небольших наборах данных
- Интерпретируемость результатов
- Поддержка различных детекторов и дескрипторов
- TF-IDF взвешивание для улучшения качества

#### Недостатки:
- Теряется пространственная информация
- Требует ручной настройки параметров

### 2. Нейросетевой классификатор с transfer learning

#### Принцип работы:

Используется подход transfer learning (переноса обучения):

1. **Базовая модель:**
   - Используются предобученные модели (ResNet50, VGG16, MobileNetV2)
   - Модели предобучены на большом наборе данных ImageNet
   - Веса базовой модели замораживаются (не обучаются)

2. **Архитектура:**
   - Базовая модель используется как экстрактор признаков
   - Добавляются дополнительные слои:
     - GlobalAveragePooling2D - усреднение пространственных признаков
     - Dropout - регуляризация для предотвращения переобучения
     - Dense(128) - полносвязный слой с ReLU активацией
     - Dropout - дополнительная регуляризация
     - Dense(num_classes) - выходной слой с softmax активацией

3. **Обучение:**
   - Используется аугментация данных (повороты, сдвиги, отражения)
   - Оптимизатор: Adam
   - Функция потерь: sparse_categorical_crossentropy
   - Метрика: accuracy

4. **Предобработка:**
   - Изменение размера до 224x224 пикселей
   - Конвертация BGR -> RGB
   - Нормализация в диапазон [0, 1]

#### Преимущества:
- Высокая точность классификации
- Автоматическое извлечение признаков
- Хорошая обобщающая способность
- Использование предобученных весов экономит время и ресурсы

#### Недостатки:
- Требует больше вычислительных ресурсов
- Больше времени на обучение
- Менее интерпретируемые результаты

## Результаты

После выполнения обучения и тестирования приложение выводит:
- Общую точность классификации (accuracy)
- Детальную статистику по каждому классу
- Сохраненные модели для последующего использования

### Примеры результатов

**Bag of Words (SIFT, vocab_size=200):**
- Общая точность: ~96%
- Нижегородский Кремль: ~100%
- Архангельский собор: ~90%
- Дворец труда: ~100%

**Нейронная сеть (ResNet50):**
- Общая точность: зависит от количества эпох и параметров обучения
- Обычно достигает 95-99% при достаточном обучении

## Технические детали

### Используемые библиотеки:
- **OpenCV** - обработка изображений и извлечение признаков
- **NumPy** - работа с массивами
- **scikit-learn** - кластеризация и классификация (для Bag of Words)
- **TensorFlow/Keras** - нейронные сети
- **Pillow** - дополнительная обработка изображений

### Структура данных:

Данные должны быть организованы следующим образом:
```
data_dir/
├── ExtDataset/
│   ├── 01_NizhnyNovgorodKremlin/
│   ├── 04_ArkhangelskCathedral/
│   └── 08_PalaceOfLabor/
└── NNSUDataset/
    ├── 01_NizhnyNovgorodKremlin/
    ├── 04_ArkhangelskCathedral/
    └── 08_PalaceOfLabor/
```

Файл `train.txt` содержит относительные пути к тренировочным изображениям, по одному на строку.

## Дополнительные возможности

### Исследование параметров

1. **Сравнение детекторов:** Используйте `compare_detectors.py` для оценки эффективности различных детекторов (SIFT, ORB, AKAZE)

2. **Размер словаря:** Экспериментируйте с параметром `--vocab_size` (рекомендуется 150-500). Для автоматического исследования используйте `research_vocab_sizes.py`

3. **Базовая модель для нейронной сети:** Попробуйте разные модели:
   - `ResNet50` - баланс скорости и точности
   - `VGG16` - более простая архитектура
   - `MobileNetV2` - быстрая и легкая модель

### Визуализация этапов работы

Bag of Words поддерживает визуализацию ключевых точек:
- Используйте флаг `--visualize` при обучении
- Результаты сохраняются в директорию `visualizations/`
- Помогает анализировать качество детектирования признаков

## Требования к системе

- Python 3.8+
- OpenCV 4.8+
- NumPy < 2.0.0 (для совместимости с TensorFlow)
- TensorFlow 2.13+ (для нейронной сети)
- scikit-learn 1.3+

**Важно:** Если возникают проблемы с TensorFlow из-за несовместимости версий NumPy, используйте:
```bash
pip install 'numpy<2.0.0,>=1.24.0' 'tensorflow>=2.13.0'
```

## Автор

Практическая работа выполнена в рамках курса по компьютерному зрению.

