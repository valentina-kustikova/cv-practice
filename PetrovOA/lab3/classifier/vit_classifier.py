import cv2
import numpy as np
import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from PIL import Image
from tqdm import tqdm
from .base_classifier import BaseClassifier


class ImageDataset(Dataset):
    """Dataset –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_path = self.image_paths[idx]
        image = cv2.imread(image_path)
        
        if image is None:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è BGR -> RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = Image.fromarray(image)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
        if self.transform:
            image = self.transform(image)
        
        label = self.labels[idx]
        
        return image, label


class ViTClassifier(BaseClassifier):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ Vision Transformer (DINOv2)
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å DINOv2-small —Å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º backbone
    –∏ –æ–±—É—á–∞–µ–º–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –≥–æ–ª–æ–≤–æ–π.
    
    –û–±—É—á–µ–Ω–∏–µ: GPU
    –ò–Ω—Ñ–µ—Ä–µ–Ω—Å: CPU
    """
    
    def __init__(self, model_dir='vit_model', image_size=224,
                 learning_rate=0.001, batch_size=16, epochs=20):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ViT –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        
        Args:
            model_dir (str): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
            image_size (int): –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            learning_rate (float): –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            batch_size (int): –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            epochs (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        """
        super().__init__(model_dir)
        self.image_size = image_size
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.epochs = epochs
        
        self.model = None
        self.label_encoder = LabelEncoder()
        self.device = None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏/–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π)
        self.train_transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
        self.test_transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
    
    def create_model(self, n_classes):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Vision Transformer
        
        Args:
            n_classes (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        """
        print("\n=== –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Vision Transformer ===")
        print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å –∏–∑ torchvision (–∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PyTorch Hub)")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ViT –∏–∑ torchvision (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
        try:
            from torchvision.models import vit_b_16, ViT_B_16_Weights
            print("\n–ó–∞–≥—Ä—É–∑–∫–∞ ViT-B/16 —Å –≤–µ—Å–∞–º–∏ ImageNet-1K...")
            backbone = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º encoder (–±–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –≥–æ–ª–æ–≤—ã)
            # ViT —Å–æ—Å—Ç–æ–∏—Ç –∏–∑: conv_proj, encoder, heads
            # –ù–∞–º –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ encoder
            class ViTEncoder(nn.Module):
                def __init__(self, vit_model):
                    super().__init__()
                    self.conv_proj = vit_model.conv_proj
                    self.encoder = vit_model.encoder
                    self.class_token = vit_model.class_token
                    
                def forward(self, x):
                    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞—Ç—á–∏
                    x = self.conv_proj(x)
                    x = x.flatten(2).transpose(1, 2)
                    
                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ class token
                    batch_size = x.shape[0]
                    class_token = self.class_token.expand(batch_size, -1, -1)
                    x = torch.cat([class_token, x], dim=1)
                    
                    # Encoder
                    x = self.encoder(x)
                    
                    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ class token (–ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω)
                    return x[:, 0]
            
            backbone = ViTEncoder(backbone)
            embedding_dim = 768  # ViT-B/16 –∏–º–µ–µ—Ç 768-–º–µ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            print("‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: ViT-B/16 (torchvision, ImageNet-1K)")
            
        except Exception as e:
            print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å ViT-B/16: {e}")
            print("  –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ResNet50 –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É...")
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: ResNet50
            from torchvision.models import resnet50, ResNet50_Weights
            backbone_full = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
            # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π FC —Å–ª–æ–π
            backbone = nn.Sequential(*list(backbone_full.children())[:-1])
            # –î–æ–±–∞–≤–ª—è–µ–º Flatten
            backbone = nn.Sequential(backbone, nn.Flatten())
            embedding_dim = 2048  # ResNet50 –∏–º–µ–µ—Ç 2048-–º–µ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            print("‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: ResNet50 (torchvision, ImageNet-1K)")
        
        # –ó–∞–º–æ—Ä–æ–∑–∫–∞ backbone (–Ω–µ –æ–±—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å)
        for param in backbone.parameters():
            param.requires_grad = False
        
        print(f"‚úì Backbone –∑–∞–º–æ—Ä–æ–∂–µ–Ω (–Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è)")
        print(f"‚úì –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embedding_dim}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –≥–æ–ª–æ–≤—ã
        classifier_head = nn.Sequential(
            nn.Linear(embedding_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, n_classes)
        )
        
        # –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
        class ViTWithHead(nn.Module):
            def __init__(self, backbone, head):
                super().__init__()
                self.backbone = backbone
                self.head = head
            
            def forward(self, x):
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è backbone)
                with torch.no_grad():
                    features = self.backbone(x)
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (—Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ –¥–ª—è –≥–æ–ª–æ–≤—ã)
                logits = self.head(features)
                return logits
        
        self.model = ViTWithHead(backbone, classifier_head)
        
        print(f"‚úì –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞ —Å–æ–∑–¥–∞–Ω–∞: {embedding_dim} -> 512 -> 256 -> {n_classes}")
        print(f"‚úì –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—É—á–µ–Ω–∏—é\n")
    
    def train(self, train_paths, train_labels):
        """
        –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ GPU
        
        Args:
            train_paths (list): –ü—É—Ç–∏ –∫ –æ–±—É—á–∞—é—â–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
            train_labels (list): –ú–µ—Ç–∫–∏ –æ–±—É—á–∞—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        print("\n" + "="*60)
        print("–û–ë–£–ß–ï–ù–ò–ï VISION TRANSFORMER –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê")
        print("="*60)
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"\nüñ•  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è: {self.device}")
        
        if self.device.type == 'cuda':
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
            print(f"   –ü–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        else:
            print("   ‚ö† GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω—ã–º!")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫
        self.class_names = sorted(list(set(train_labels)))
        encoded_labels = self.label_encoder.fit_transform(train_labels)
        
        print(f"\nüìä –î–∞–Ω–Ω—ã–µ:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(train_paths)}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(self.class_names)}")
        print(f"   –ö–ª–∞—Å—Å—ã: {self.class_names}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if self.model is None:
            self.create_model(len(self.class_names))
        
        self.model = self.model.to(self.device)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
        dataset = ImageDataset(train_paths, encoded_labels, transform=self.train_transform)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
        train_size = int(0.8 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
        val_dataset.dataset.transform = self.test_transform
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=0  # Windows —á–∞—Å—Ç–æ –∏–º–µ–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å num_workers > 0
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=0
        )
        
        print(f"\nüì¶ –ë–∞—Ç—á–∏:")
        print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {self.batch_size}")
        print(f"   Train –±–∞—Ç—á–µ–π: {len(train_loader)}")
        print(f"   Val –±–∞—Ç—á–µ–π: {len(val_loader)}")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        # –û–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—É—é –≥–æ–ª–æ–≤—É
        optimizer = optim.Adam(
            self.model.head.parameters(),  # –¢–æ–ª—å–∫–æ –≥–æ–ª–æ–≤–∞!
            lr=self.learning_rate
        )
        criterion = nn.CrossEntropyLoss()
        
        # Scheduler –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è learning rate
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=3
        )
        
        print(f"\n‚öô  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
        print(f"   Learning rate: {self.learning_rate}")
        print(f"   Epochs: {self.epochs}")
        print(f"   Scheduler: ReduceLROnPlateau (patience=3, factor=0.5)")
        print(f"   –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam (—Ç–æ–ª—å–∫–æ –≥–æ–ª–æ–≤–∞)")
        
        # –û–±—É—á–µ–Ω–∏–µ
        best_val_acc = 0.0
        train_losses = []
        val_accuracies = []
        
        print(f"\n{'='*60}")
        print("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
        print(f"{'='*60}\n")
        
        for epoch in range(self.epochs):
            print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{self.epochs}")
            print("-" * 60)
            
            # === –û–ë–£–ß–ï–ù–ò–ï ===
            self.model.train()
            running_loss = 0.0
            correct = 0
            total = 0
            
            train_bar = tqdm(train_loader, desc="Training", leave=False)
            for images, labels in train_bar:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # Forward pass
                optimizer.zero_grad()
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                
                # Backward pass
                loss.backward()
                optimizer.step()
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                train_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{100 * correct / total:.2f}%'
                })
            
            epoch_loss = running_loss / len(train_loader)
            train_acc = 100 * correct / total
            train_losses.append(epoch_loss)
            
            # === –í–ê–õ–ò–î–ê–¶–ò–Ø ===
            self.model.eval()
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                val_bar = tqdm(val_loader, desc="Validation", leave=False)
                for images, labels in val_bar:
                    images = images.to(self.device)
                    labels = labels.to(self.device)
                    
                    outputs = self.model(images)
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
                    
                    val_bar.set_postfix({
                        'acc': f'{100 * val_correct / val_total:.2f}%'
                    })
            
            val_acc = 100 * val_correct / val_total
            val_accuracies.append(val_acc)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate
            old_lr = optimizer.param_groups[0]['lr']
            scheduler.step(val_acc)
            new_lr = optimizer.param_groups[0]['lr']
            
            # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            print(f"Train Loss: {epoch_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")
            
            # –ï—Å–ª–∏ LR –∏–∑–º–µ–Ω–∏–ª—Å—è, –≤—ã–≤–æ–¥–∏–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
            if new_lr != old_lr:
                print(f"‚ö† Learning rate —Å–Ω–∏–∂–µ–Ω: {old_lr:.6f} ‚Üí {new_lr:.6f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                self.save_model()
                print(f"‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ª—É—á—à–∞—è val acc: {best_val_acc:.2f}%)")
            
            print()
        
        print(f"{'='*60}")
        print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print(f"{'='*60}")
        print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_acc:.2f}%")
        print()
    
    def test(self, test_paths, test_labels=None):
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ CPU
        
        Args:
            test_paths (list): –ü—É—Ç–∏ –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
            test_labels (list, optional): –ú–µ—Ç–∫–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        
        Returns:
            tuple: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å
        """
        print("\n" + "="*60)
        print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï VISION TRANSFORMER –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê")
        print("="*60)
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (CPU –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞)
        self.device = torch.device('cpu')
        print(f"\nüñ•  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {self.device}")
        
        if self.model is None:
            self.load_model()
        
        self.model = self.model.to(self.device)
        self.model.eval()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
        # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º dummy labels –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        dummy_labels = [0] * len(test_paths) if test_labels is None else self.label_encoder.transform(test_labels)
        
        dataset = ImageDataset(test_paths, dummy_labels, transform=self.test_transform)
        test_loader = DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=0
        )
        
        print(f"\nüìä –î–∞–Ω–Ω—ã–µ:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(test_paths)}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π: {len(test_loader)}")
        
        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
        all_predictions = []
        
        print(f"\n{'='*60}")
        print("–ù–ê–ß–ê–õ–û –ò–ù–§–ï–†–ï–ù–°–ê")
        print(f"{'='*60}\n")
        
        with torch.no_grad():
            test_bar = tqdm(test_loader, desc="Testing")
            for images, _ in test_bar:
                images = images.to(self.device)
                
                outputs = self.model(images)
                _, predicted = torch.max(outputs.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        predictions = self.label_encoder.inverse_transform(all_predictions)
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        accuracy = None
        if test_labels is not None:
            accuracy = accuracy_score(test_labels, predictions)
            
            print(f"\n{'='*60}")
            print("–†–ï–ó–£–õ–¨–¢–ê–¢–´")
            print(f"{'='*60}")
            print(f"\n‚úì –¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {accuracy:.4f} ({accuracy*100:.2f}%)\n")
            
            # –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç
            report = classification_report(
                test_labels, predictions,
                target_names=self.class_names,
                digits=4
            )
            print("–û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º:")
            print(report)
        
        return predictions, accuracy
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª"""
        if not os.path.exists(self.model_dir):
            os.makedirs(self.model_dir)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏
        model_path = os.path.join(self.model_dir, 'vit_model.pth')
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'class_names': self.class_names,
            'label_encoder_classes': self.label_encoder.classes_.tolist(),
            'image_size': self.image_size
        }, model_path)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ JSON
        metadata = {
            'class_names': self.class_names,
            'image_size': self.image_size,
            'learning_rate': self.learning_rate,
            'batch_size': self.batch_size,
            'epochs': self.epochs,
            'model_type': 'DINOv2-ViT-Small'
        }
        
        metadata_path = os.path.join(self.model_dir, 'metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        model_path = os.path.join(self.model_dir, 'vit_model.pth')
        
        if not os.path.exists(model_path):
            abs_path = os.path.abspath(model_path)
            raise ValueError(
                f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {abs_path}\n"
                f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —Ñ–∞–π–ª vit_model.pth —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: "
                f"{os.path.abspath(self.model_dir)}"
            )
        
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        self.class_names = checkpoint['class_names']
        self.image_size = checkpoint.get('image_size', self.image_size)
        
        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ label encoder
        self.label_encoder = LabelEncoder()
        self.label_encoder.classes_ = np.array(checkpoint['label_encoder_classes'])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if self.model is None:
            self.create_model(len(self.class_names))
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        print(f"  –ö–ª–∞—Å—Å—ã: {self.class_names}")
