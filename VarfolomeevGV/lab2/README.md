# Практическая работа №2. Детектирование объектов на изображениях с использованием библиотеки OpenCV

## Оглавление

- [Описание проекта](#описание-проекта)
- [Структура проекта](#структура-проекта)
- [Установка](#установка)
- [Использование](#использование)
  - [Базовое использование](#базовое-использование)
  - [Параметры командной строки](#параметры-командной-строки)
  - [Примеры](#примеры)
- [Описание реализованных алгоритмов](#описание-реализованных-алгоритмов)
  - [YOLOv5](#yolov5)
  - [Faster R-CNN](#faster-r-cnn)
  - [RetinaNet](#retinanet)
- [Метрики качества](#метрики-качества)
  - [Отображение метрик](#отображение-метрик)
  - [Результаты тестирования моделей](#результаты-тестирования-моделей)
- [Классы транспортных средств](#классы-транспортных-средств)
- [Визуализация](#визуализация)
  - [Отладочный вывод (`--debug`)](#отладочный-вывод---debug)

## Описание проекта

Данный проект представляет собой библиотеку для детектирования транспортных средств на изображениях с использованием трех различных моделей нейронных сетей:
- **YOLOv5**
- **Faster R-CNN**
- **RetinaNet** 

Все модели используют модуль DNN библиотеки OpenCV для инференса и поддерживают детектирование классов транспортных средств из датасета COCO (велосипеды, автомобили, мотоциклы, автобусы, грузовики).

## Структура проекта

```
lab2/
├── detectors/                      # Библиотека детекторов объектов
│   ├── __init__.py                # Инициализация модуля детекторов
│   ├── base_detector.py          # Базовый абстрактный класс для всех детекторов
│   ├── yolo_detector.py          # Реализация детектора YOLOv5
│   ├── faster_rcnn_detector.py   # Реализация детектора Faster R-CNN
│   └── retinanet_detector.py     # Реализация детектора RetinaNet
├── utils/                         # Вспомогательные утилиты
│   ├── __init__.py               # Инициализация модуля утилит
│   ├── nms.py                    # Реализация Non-Maximum Suppression (NMS)
│   ├── metrics.py                # Вычисление метрик качества (TPR, FDR)
│   └── visualization.py          # Функции для визуализации результатов детектирования
├── models/                       # Папка для загруженных моделей нейронных сетей
│   ├── coco.names                # Файл с названиями классов COCO
│   ├── yolo/                     # Модели YOLOv5
│   │   └──  yolov5s.onnx         # ONNX модель YOLOv5s
│   ├── faster_rcnn/              # Модели Faster R-CNN
│   │   ├── frozen_inference_graph.pb  # Замороженный граф TensorFlow
│   │   └── faster_rcnn_inception_v2_coco_2018_01_28.pbtxt  # Конфигурация для OpenCV
│   └── retinanet/                # Модели RetinaNet
│       └── retinanet-9.onnx      # ONNX модель RetinaNet
├── output/                       # Папка для сохранения результатов
├── download_models.py           # Скрипт для автоматической загрузки моделей
├── demo.py                       # Консольное приложение для детектирования
├── README.md                     # Документация проекта
└── requirements.txt              # Список зависимостей Python
```

## Установка

1. Установите зависимости:
```bash
pip install -r requirements.txt
```

2. Загрузите модели:
```bash
python download_models.py
```

Скрипт автоматически:
- Загружает YOLOv5s с [GitHub Ultralytics](https://github.com/ultralytics/yolov5/releases)
- Загружает Faster-R-CNNv2 c [TensorFlow](http://download.tensorflow.org/models/object_detection/)
- Загружает RetinaNet c [Github ONNX Model Zoo](https://github.com/onnx/models)

## Использование

### Базовое использование

```bash
python demo.py --images path/to/images --model yolo --show
```

### Параметры командной строки

#### Обязательные параметры

- `--images` (обязательно): Путь к директории с изображениями для обработки.
  ```bash
  --images data/images
  ```

- `--model` (обязательно): Модель для детектирования. Доступные значения: `yolo`, `faster_rcnn`, `retinanet`
  ```bash
  --model yolo
  ```

#### Опциональные параметры модели

- `--weights`: Путь к файлу весов модели. Если не указан, используется стандартный путь для выбранной модели:
  - YOLO: `models/yolo/yolov5s.onnx`
  - Faster R-CNN: `models/faster_rcnn/frozen_inference_graph.pb`
  - RetinaNet: `models/retinanet/retinanet-9.onnx`
  ```bash
  --weights models/yolo/custom_model.onnx
  ```

- `--config`: Путь к конфигурационному файлу (требуется только для Faster R-CNN). По умолчанию: `models/faster_rcnn/faster_rcnn_inception_v2_coco_2018_01_28.pbtxt`
  ```bash
  --config models/faster_rcnn/custom_config.pbtxt
  ```

- `--names`: Путь к файлу с названиями классов. По умолчанию: `models/coco.names`
  ```bash
  --names models/custom_names.txt
  ```

#### Параметры детектирования

- `--conf-threshold`: Порог уверенности для фильтрации детекций. Детекции с уверенностью ниже этого порога отбрасываются. По умолчанию: `0.5`. Диапазон: `[0.0, 1.0]`
  ```bash
  --conf-threshold 0.3  # низкий порог - больше детекций
  --conf-threshold 0.7  # высокий порог - меньше, но остаются более точные детекции
  ```

- `--nms-threshold`: Порог Intersection over Union (IoU) для алгоритма Non-Maximum Suppression. Детекции с IoU выше этого порога считаются дубликатами, и остается только детекция с наибольшей уверенностью. По умолчанию: `0.4`. Диапазон: `[0.0, 1.0]`
  ```bash
  --nms-threshold 0.3  # Более агрессивная фильтрация дубликатов
  --nms-threshold 0.5  # Более мягкая фильтрация
  ```

#### Параметры разметки и метрик

- `--annotations`: Путь к файлу или директории с разметкой (ground truth) для вычисления метрик качества. Поддерживаются форматы:
  - **Формат 'required'**: Текстовый файл в формате `frame_id class_name x1 y1 x2 y2`, где `frame_id` - номер кадра (начинается с 0), `class_name` - название класса (CAR, BUS, TRUCK, BICYCLE, MOTORCYCLE), координаты в пикселях
    ```
    0 CAR 100 200 300 400
    0 BUS 500 100 700 300
    1 CAR 150 250 350 450
    ```
  - **Формат 'simple'**: Текстовый файл в формате `x1 y1 x2 y2 class_id` (одна строка на объект)
  - Директория с отдельными файлами разметки для каждого изображения
  ```bash
  --annotations data/annotations.txt
  --annotations data/annotations/  # Директория с файлами разметки
  ```
  
  **Примечание**: При использовании формата 'required' разметка автоматически фильтруется по номеру кадра (frame_id), соответствующему индексу изображения в отсортированном списке.

#### Параметры вывода

- `--show`: Флаг для отображения изображений с детекциями в окне. При использовании этого флага программа будет показывать каждое обработанное изображение в отдельном окне
  ```bash
  --show
  ```

- `--save`: Флаг для сохранения обработанных изображений с детекциями. Изображения сохраняются в директорию, указанную в `--output`
  ```bash
  --save
  ```

- `--output`: Директория для сохранения результатов (обработанных изображений и видео). По умолчанию: `output/`
  ```bash
  --output results/
  ```

#### Параметры видео

- `--vid`: Флаг для создания видео из обработанных кадров. Видео сохраняется в формате MP4 в директории `--output` с именем `output_video.mp4`
  ```bash
  --vid
  ```

- `--fps`: Частота кадров для создаваемого видео. По умолчанию: `25`
  ```bash
  --fps 30
  ```

- `--duration`: Количество кадров для обработки. Если указано `0` (по умолчанию), обрабатываются все кадры. Для видео по умолчанию ограничивается до 100 кадров
  ```bash
  --duration 50  # Обработать только первые 50 кадров
  ```

#### Параметры просмотра

- `--quick`: Флаг для автоматического переключения кадров при просмотре с задержкой 100 мс. Без этого флага программа ждет нажатия клавиши для перехода к следующему кадру
  ```bash
  --show --quick  # Автоматическое переключение кадров
  ```

### Примеры

```bash
# Детектирование с YOLOv5 и сохранение результатов
python demo.py --images data/images --model yolo --save --output results/

# Детектирование и сохранение результатов в видеоформате (фреймрейт: 25, длительность 125 {125 / 25 = 5 секунд})
python demo.py --images data/images --model yolo --save --vid --fps 25 --duration 125 --output results/

# Детектирование и демонстрация 13 начиная с начала
python demo.py --images data/images --model yolo --show --duration 13 --output results/

# Детектирование с Faster R-CNN и вычисление метрик
python demo.py --images data/images --model faster_rcnn --annotations data/annotations.txt

# Детектирование с RetinaNet и отображение результатов с автоматическим переключением
python demo.py --images data/images --model retinanet --show --quick
```

## Описание реализованных алгоритмов

### YOLOv5

YOLOv5 (You Only Look Once ver. 5) — это однопроходный детектор объектов, характеризующийся высокой скоростью работы и хорошим балансом между точностью и производительностью. Модель использует архитектуру с единой нейронной сетью, которая одновременно предсказывает координаты объектов и их классы, что делает её особенно эффективной для задач реального времени.

#### Предобработка изображений

Алгоритм предобработки для YOLOv5 включает следующие этапы:

1. **Letterbox resize**: Изображение изменяется с сохранением пропорций до размера 640×640 пикселей. Вычисляется масштаб как минимум из соотношений целевой высоты к исходной высоте и целевой ширины к исходной ширине:
   ```
   scale = min(640 / height, 640 / width)
   new_height = height * scale
   new_width = width * scale
   ```

2. **Padding**: Для получения квадратного изображения 640×640 добавляется padding серого цвета (114, 114, 114) сверху/снизу или слева/справа:
   ```
   pad_h = (640 - new_height) // 2
   pad_w = (640 - new_width) // 2
   ```

3. **Нормализация**: Значения пикселей нормализуются в диапазон [0, 1] путем деления на 255.0:
   ```
   normalized = pixel_value / 255.0
   ```

4. **Преобразование цветового пространства**: Изображение преобразуется из BGR в RGB. Это необходимо, так как модель YOLOv5 обучена на изображениях в формате RGB:
   ```
   rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)
   ```

5. **Преобразование формата**: Изображение преобразуется в формат NCHW (batch, channels, height, width) для подачи в сеть:
   ```
   blob shape: [1, 3, 640, 640]
   ```

#### Постобработка выхода сети

Алгоритм постобработки для YOLOv5:

1. **Объединение выходов**: Если сеть возвращает несколько выходов (для разных масштабов), они объединяются.

2. **Извлечение компонентов**: Из выхода сети извлекаются:
   - Координаты боксов в формате YOLO: `[x_center, y_center, width, height]` (нормализованные относительно 640×640)
   - Уверенность (confidence): вероятность наличия объекта
   - Вероятности классов: вероятности принадлежности к каждому из 80 классов COCO

3. **Вычисление итоговой уверенности**: 
   ```
   final_score = confidence * max(class_probabilities)
   ```

4. **Фильтрация по confidence**: Удаляются детекции с итоговой уверенностью ниже порога (по умолчанию 0.5).

5. **Преобразование координат**: Координаты преобразуются из формата YOLO (center, width, height) в формат [x1, y1, x2, y2]:
   ```
   x1 = x_center - width / 2
   y1 = y_center - height / 2
   x2 = x_center + width / 2
   y2 = y_center + height / 2
   ```

6. **Масштабирование к исходному размеру**: Координаты масштабируются обратно к исходному размеру изображения с учетом padding:
   ```
   x1 = (x1 - pad_w) / scale
   y1 = (y1 - pad_h) / scale
   x2 = (x2 - pad_w) / scale
   y2 = (y2 - pad_h) / scale
   ```

7. **Ограничение координат**: Координаты ограничиваются границами исходного изображения.

8. **Non-Maximum Suppression (NMS)**: Применяется алгоритм NMS для удаления дублирующихся детекций одного объекта. Алгоритм:
   - Сортирует детекции по убыванию уверенности
   - Для каждой детекции вычисляет IoU с остальными
   - Удаляет детекции с IoU выше порога (по умолчанию 0.4)

**Формат выходных данных**: Список детекций в формате `[x1, y1, x2, y2, class_id, confidence]`, где координаты в пикселях исходного изображения.

---

### Faster R-CNN

Faster R-CNN (Region-based Convolutional Neural Network) — это двухэтапный детектор объектов, известный своей высокой точностью детектирования. Архитектура состоит из двух основных компонентов: Region Proposal Network (RPN) для генерации предложений регионов и ROI Head для классификации и уточнения координат. Модель обеспечивает высокую точность за счет более сложной архитектуры, но работает медленнее однопроходных детекторов.

#### Предобработка изображений

Алгоритм предобработки для Faster R-CNN:

1. **Resize с сохранением пропорций**: Изображение изменяется с сохранением пропорций, при этом максимальная сторона ограничивается 800 пикселями:
   ```
   max_size = 800
   scale = min(max_size / height, max_size / width, 1.0)
   new_height = height * scale
   new_width = width * scale
   ```
   Примечание: маленькие изображения не увеличиваются (scale ≤ 1.0).

2. **Нормализация ImageNet**: Из значений пикселей вычитается среднее значение ImageNet для BGR каналов:
   ```
   mean = [103.939, 116.779, 123.68]  # BGR
   normalized = pixel_value - mean
   ```
    Модель Faster R-CNN использует backbone (Inception v2), который был предобучен на ImageNet с такой нормализацией.

3. **Преобразование формата**: Изображение преобразуется в формат NCHW:
   ```
   blob shape: [1, 3, new_height, new_width]
   ```

#### Постобработка выхода сети

Алгоритм постобработки для Faster R-CNN:

1. **Извлечение данных**: Выход Faster R-CNN обычно уже в готовом формате `[batch, num_detections, 7]`, где 7 значений:
   - `[0]`: batch_id (обычно 0)
   - `[1]`: class_id (идентификатор класса)
   - `[2]`: confidence (уверенность)
   - `[3:7]`: координаты `[x1, y1, x2, y2]` в пикселях обработанного изображения

2. **Фильтрация по confidence**: Удаляются детекции с уверенностью ниже порога (по умолчанию 0.5).

3. **Масштабирование координат**: Координаты масштабируются обратно к исходному размеру изображения:
   ```
   x1 = x1 / scale
   y1 = y1 / scale
   x2 = x2 / scale
   y2 = y2 / scale
   ```

4. **Ограничение координат**: Координаты ограничиваются границами исходного изображения.

**Особенности**: Faster R-CNN уже применяет NMS внутри модели, поэтому дополнительный NMS обычно не требуется. Выходные детекции уже отсортированы по уверенности.

**Формат выходных данных**: Список детекций в формате `[x1, y1, x2, y2, class_id, confidence]`, где координаты в пикселях исходного изображения.

---

### RetinaNet

RetinaNet — это однопроходный детектор объектов, разработанный для решения проблемы дисбаланса классов при обучении детекторов. Модель использует архитектуру Feature Pyramid Network (FPN) для обработки объектов разных масштабов и функцию потерь Focal Loss для эффективного обучения на сложных примерах. RetinaNet сочетает скорость однопроходных детекторов с точностью, близкой к двухэтапным моделям.

#### Предобработка изображений

Алгоритм предобработки для RetinaNet включает следующие этапы:

1. **Letterbox resize**: Изображение изменяется с сохранением пропорций до размера 640×640 пикселей. Вычисляется масштаб как минимум из соотношений целевой высоты к исходной высоте и целевой ширины к исходной ширине:
   ```
   scale = min(640 / height, 640 / width)
   new_height = height * scale
   new_width = width * scale
   ```

2. **Padding**: Для получения квадратного изображения 640×640 добавляется padding черного цвета (0, 0, 0) сверху/снизу или слева/справа:
   ```
   pad_h = (640 - new_height) // 2
   pad_w = (640 - new_width) // 2
   ```

3. **Нормализация ImageNet**: Применяется нормализация ImageNet с вычитанием среднего значения и делением на стандартное отклонение для BGR каналов:
   ```
   mean = [103.939, 116.779, 123.68]  # BGR
   std = [58.395, 57.12, 57.375]      # BGR
   normalized = (pixel_value - mean) / std
   ```
   Это стандартная нормализация для моделей, обученных на ImageNet.

4. **Преобразование формата**: Изображение преобразуется в формат NCHW (batch, channels, height, width) для подачи в сеть:
   ```
   blob shape: [1, 3, 640, 640]
   ```

#### Постобработка выхода сети

RetinaNet использует архитектуру Feature Pyramid Network (FPN) и может возвращать выходы в разных форматах. Алгоритм постобработки зависит от формата выхода:

**Вариант 1: Один выходной тензор [N, 6]**

Если модель возвращает один тензор формата `[N, 6]`, где каждая строка содержит `[x1, y1, x2, y2, class_id, score]`:

1. **Извлечение компонентов**: Из выхода извлекаются координаты боксов, идентификаторы классов и уверенности.
2. **Фильтрация по confidence**: Удаляются детекции с уверенностью ниже порога (по умолчанию 0.5).
3. **Преобразование координат**: Координаты преобразуются из формата входного изображения модели (640×640 с padding) в координаты исходного изображения:
   - Убирается padding: `x = x - pad_w`, `y = y - pad_h`
   - Масштабирование: `x = x / scale`, `y = y / scale`
4. **Ограничение координат**: Координаты ограничиваются границами исходного изображения.
5. **Non-Maximum Suppression (NMS)**: Применяется алгоритм NMS для удаления дублирующихся детекций.

**Вариант 2: FPN выходы (10 тензоров)**

Если модель возвращает 10 выходов (5 уровней FPN × 2 типа выходов):

1. **Разделение выходов**: Выходы разделяются на боксы (36 каналов = 9 anchors × 4 координаты) и оценки классов (720 каналов = 9 anchors × 80 классов).
2. **Обработка каждого уровня FPN**:
   - Генерация якорей для текущего уровня feature map
   - Декодирование предсказаний боксов относительно якорей
   - Применение sigmoid к логитам классов для получения вероятностей
   - Фильтрация по confidence
3. **Объединение детекций**: Все детекции с разных уровней объединяются.
4. **Фильтрация дубликатов**: Применяется дополнительная фильтрация дубликатов по близости центров и IoU.
5. **NMS**: Применяется Non-Maximum Suppression для финальной фильтрации.

**Декодирование боксов**: RetinaNet использует параметризацию смещений относительно якорей:
```
dx, dy = смещения центра (нормализованные на размеры якоря)
dw, dh = логарифмические изменения размеров
pred_x = anchor_x + dx * anchor_w
pred_y = anchor_y + dy * anchor_h
pred_w = anchor_w * exp(dw)
pred_h = anchor_h * exp(dh)
```

**Особенности обработки координат**:
- Код автоматически определяет формат выходов модели (нормализованные координаты [0,1] или пиксели)
- Координаты правильно преобразуются из системы координат модели в систему координат исходного изображения
- Учитывается padding и масштабирование при преобразовании координат

**Формат выходных данных**: Список детекций в формате `[x1, y1, x2, y2, class_id, confidence]`, где координаты в пикселях исходного изображения.

---

## Метрики качества

Приложение вычисляет следующие метрики качества детектирования:

- **TPR (True Positive Rate)** - Полнота: доля правильно найденных объектов от общего числа объектов в разметке.
  ```
  TPR = TP / (TP + FN)
  ```
  где TP (True Positive) - правильно найденные объекты, FN (False Negative) - пропущенные объекты.

- **FDR (False Discovery Rate)** - Ложная частота обнаружения: доля ложных срабатываний от общего числа найденных объектов.
  ```
  FDR = FP / (FP + TP)
  ```
  где FP (False Positive) - ложные срабатывания.

Для вычисления метрик используется порог IoU (Intersection over Union) равный 0.5 - детекция считается правильной, если IoU с истинным объектом ≥ 0.5 и класс совпадает.

### Отображение метрик

При наличии разметки (`--annotations`) метрики TPR и FDR автоматически отображаются в верхнем левом углу каждого обработанного изображения:

В конце обработки всех изображений в консоль выводится среднее значение метрик по всем кадрам.

### Результаты тестирования моделей

Результаты тестирования на датасете MOV03478 (100 кадров) при различных порогах уверенности:

#### YOLOv5

| Порог уверенности | TPR (среднее) | FDR (среднее) |
|-------------------|---------------|---------------|
| 0.5               | 0.966             | 0.099             |
| 0.6               | 0.928             | 0.066             |
| 0.7               | 0.862             | 0.033             |

#### Faster R-CNN

| Порог уверенности | TPR (среднее) | FDR (среднее) |
|-------------------|---------------|---------------|
| 0.5               | 0.942             | 0.047             |
| 0.6               | 0.91             | 0.021             |
| 0.7               | 0.836             | 0.014             |

#### RetinaNet

| Порог уверенности | TPR (среднее) | FDR (среднее) |
|-------------------|---------------|---------------|
| 0.5               | 0.990          | 0.681         |
| 0.6               | 0.932             | 0.022             |
| 0.7               | 0.565             | 0             |

**Команда для тестирования:**
```bash
python demo.py --images "путь/к/изображениям" --model {yolo|faster_rcnn|retinanet} --quick --duration 100 --conf-threshold {0.5|0.6|0.7} --annotations "путь/к/разметке.txt"
```

## Классы транспортных средств

Библиотека фильтрует детекции по классам транспортных средств из датасета COCO (индексы начинаются с 0):

- **1**: bicycle
- **2**: car
- **3**: motorcycle
- **5**: bus
- **7**: truck

## Визуализация

При визуализации результатов детектирования:

- Каждый класс объектов отображается прямоугольником своего цвета.
- В левом верхнем углу каждого прямоугольника выводится название класса и уверенность в формате `класс: 0.xxx` с точностью до трёх знаков после запятой.
- Над прямоугольником выводится название класса объекта.
- Цвета генерируются автоматически для каждого класса.
- При наличии разметки в верхнем левом углу изображения отображаются метрики TPR и FDR.

### Отладочный вывод (`--debug`)

При вычислении метрик выводится отладочная информация:
- Классы в предсказаниях модели
- Классы в разметке (ground truth)
- Предупреждение, если классы не совпадают

Это помогает диагностировать проблемы с сопоставлением предсказаний и разметки.
