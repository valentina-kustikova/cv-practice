# Практическая работа №2. Детектирование объектов на изображениях с использованием библиотеки OpenCV

## Описание проекта

Данная работа представляет собой приложение для детектирования транспортных средств на изображениях с использованием обученных нейронных сетей из "зоопарка" моделей OpenCV. Приложение поддерживает несколько моделей детектирования объектов и позволяет оценивать качество детектирования с помощью метрик TPR (True Positive Rate) и FDR (False Discovery Rate).

## Структура проекта

```
.
├── main.py                      # Главный файл приложения
├── config.py                    # Конфигурация моделей и классов
├── detectors/                   # Модуль детекторов
│   ├── base_detector.py        # Базовый класс детектора
│   ├── ssd_detector.py         # Детектор SSD MobileNet
│   ├── yolov_detector.py       # Детектор YOLOv4
│   ├── faster_rcnn_detector.py # Детектор Faster R-CNN
├── utils/                       # Вспомогательные модули
│   └── metrics.py              # Расчет метрик качества
├── models/                      # Директория с моделями
│   ├── coco.names              # Список классов COCO
│   ├── yolov4.weights          # Веса YOLOv4
│   ├── yolov4.cfg              # Конфигурация YOLOv4
│   ├── mobilenet_iter_73000.caffemodel  # Модель SSD MobileNet
│   ├── deploy.prototxt         # Конфигурация SSD MobileNet
│   ├── faster_rcnn_inception_v2_coco_2018_01_28/  # Директория Faster R-CNN
└── README.md                    # Данный файл
```

## Использование

### Запуск приложения

```bash
python main.py --images_path <путь_к_изображениям> --annotation_file <путь_к_аннотациям> --model <тип_модели> [опции]
```

### Параметры командной строки

- `--images_path` (обязательный) - путь к директории с изображениями
- `--annotation_file` (обязательный) - путь к файлу с аннотациями (формат: "frame_number CLASS x1 y1 x2 y2")
- `--model` (обязательный) - тип модели: `yolo`, `ssd`, `faster_rcnn`
- `--conf_threshold` (опционально) - порог уверенности для детектирования (по умолчанию: 0.5)
- `--show_display` (опционально) - отображать результаты в окне
- `--save_output` (опционально) - сохранять обработанные изображения
- `--output_path` (опционально) - путь для сохранения результатов (по умолчанию: `output`)
- `--vehicle_only` (опционально) - детектировать только транспортные средства (по умолчанию: True)

### Пример использования

```bash
python main.py --images_path imgs_MOV03478 --annotation_file mov03478.txt --model yolo --conf_threshold 0.5 --show_display --save_output
```

## Архитектура решения

Проект реализует иерархию классов для детектирования объектов. Базовый класс `BaseDetector` определяет общий интерфейс, а конкретные детекторы наследуются от него и реализуют специфичные методы предобработки и постобработки для каждой модели.

### Базовый класс BaseDetector

Базовый класс определяет общий интерфейс для всех детекторов:
- `load_model()` - загрузка модели
- `preprocess()` - предобработка изображения
- `postprocess()` - постобработка выхода сети
- `detect()` - основной метод детектирования

## Описание моделей и алгоритмов обработки

### 1. YOLOv4 (You Only Look Once v4)

**Формат модели:** Darknet (.weights, .cfg)

**Алгоритм предобработки изображений:**
1. Изображение преобразуется в blob с помощью `cv2.dnn.blobFromImage()`
2. Параметры предобработки:
   - `scalefactor`: 1/255.0 (нормализация пикселей в диапазон [0, 1])
   - `size`: (416, 416) (изображение масштабируется до фиксированного размера)
   - `mean`: (0, 0, 0) (без вычитания среднего значения)
   - `swapRB`: True (конвертация BGR → RGB)
   - `crop`: False (без обрезки, сохраняются пропорции)

**Алгоритм постобработки выхода сети:**
1. Выход сети представляет собой массив детекций, где каждая детекция содержит:
   - Координаты центра объекта (center_x, center_y) и размеры (width, height) в нормализованном виде [0, 1]
   - Уверенность наличия объекта (objectness confidence)
   - Вероятности для каждого из 80 классов COCO
2. Для каждой детекции:
   - Вычисляется общая уверенность как произведение objectness confidence и максимальной вероятности класса
   - Координаты преобразуются из нормализованных в абсолютные (умножение на размеры исходного изображения)
   - Координаты центра преобразуются в координаты углов прямоугольника: `x = center_x - width/2`, `y = center_y - height/2`
3. Фильтрация по порогу уверенности (`conf_threshold`)
4. Применение Non-Maximum Suppression (NMS) с порогом IoU 0.4 для удаления дублирующихся детекций
5. Маппинг ID класса на название из файла `coco.names`

**Особенности:**
- Модель детектирует 80 классов объектов из датасета COCO
- Использует многоуровневую архитектуру с тремя масштабами детектирования
- Высокая точность детектирования, но относительно медленная скорость работы

---

### 2. SSD MobileNet V2

**Формат модели:** Caffe (.caffemodel, .prototxt)

**Алгоритм предобработки изображений:**
1. Изображение преобразуется в blob с помощью `cv2.dnn.blobFromImage()`
2. Параметры предобработки:
   - `scalefactor`: 0.007843 (эквивалентно 1/127.5, нормализация в диапазон [-1, 1])
   - `size`: (300, 300) (изображение масштабируется до фиксированного размера)
   - `mean`: (127.5, 127.5, 127.5) (вычитание среднего значения для нормализации)
   - `swapRB`: True (конвертация BGR → RGB)
   - `crop`: False (без обрезки)

**Алгоритм постобработки выхода сети:**
1. Выход сети имеет формат `[1, 1, N, 7]`, где N - количество детекций (обычно 100)
2. Каждая детекция представлена массивом из 7 элементов:
   - `[0]`: batch_id (всегда 0)
   - `[1]`: class_id (ID класса из VOC датасета, 20 классов)
   - `[2]`: confidence (уверенность детекции)
   - `[3-6]`: координаты bbox в нормализованном виде [x1, y1, x2, y2] в диапазоне [0, 1]
3. Для каждой детекции:
   - Пропускаются детекции с `class_id = 0` (background класс)
   - Фильтрация по порогу уверенности
   - Координаты преобразуются из нормализованных в абсолютные (умножение на размеры исходного изображения)
   - Маппинг ID класса из VOC в COCO имена (например, VOC class_id 7 → COCO 'car')
4. Валидация координат (проверка на корректность размеров и границ)
5. Возврат списка детекций с COCO именами классов

**Особенности:**
- Модель обучена на датасете VOC (20 классов), но классы маппятся на COCO имена для совместимости
- Быстрая скорость работы благодаря архитектуре MobileNet
- Меньшая точность по сравнению с YOLO, но достаточная для многих приложений

---

### 3. Faster R-CNN (Inception V2)

**Формат модели:** TensorFlow (.pb, .pbtxt)

**Алгоритм предобработки изображений:**
1. Изображение преобразуется в blob с помощью `cv2.dnn.blobFromImage()`
2. Параметры предобработки:
   - `scalefactor`: 1.0 (без нормализации, значения остаются в диапазоне [0, 255])
   - `size`: (300, 300) (изображение масштабируется до фиксированного размера)
   - `mean`: (0, 0, 0) (без вычитания среднего значения)
   - `swapRB`: True (конвертация BGR → RGB)
   - `crop`: False (без обрезки)

**Алгоритм постобработки выхода сети:**
1. Выход сети имеет формат `[1, 1, N, 7]`, где N - количество детекций (обычно 100)
2. Каждая детекция представлена массивом из 7 элементов:
   - `[0]`: batch_id (всегда 0)
   - `[1]`: class_id (ID класса из COCO датасета, 80 классов)
   - `[2]`: confidence (уверенность детекции)
   - `[3-6]`: координаты bbox в нормализованном виде [x1, y1, x2, y2] в диапазоне [0, 1]
3. Для каждой детекции:
   - Фильтрация по порогу уверенности
   - Координаты преобразуются из нормализованных в абсолютные
   - Проверка валидности координат (x1 < x2, y1 < y2, координаты в пределах изображения)
   - Маппинг ID класса на название из файла `coco.names`
4. Возврат списка детекций

**Особенности:**
- Двухэтапная архитектура: сначала генерируются регионы интереса (RPN), затем классификация
- Высокая точность детектирования, особенно для малых объектов
- Относительно медленная скорость работы по сравнению с однопроходными детекторами

---

## Метрики качества

Приложение вычисляет следующие метрики качества детектирования:

- **TPR (True Positive Rate)** - доля правильно детектированных объектов от общего количества объектов в ground truth:
  ```
  TPR = TP / (TP + FN)
  ```
  где TP (True Positives) - правильно детектированные объекты, FN (False Negatives) - пропущенные объекты.

- **FDR (False Discovery Rate)** - доля ложных срабатываний от общего количества детекций:
  ```
  FDR = FP / (TP + FP)
  ```
  где FP (False Positives) - ложные срабатывания.

Для сопоставления детекций с ground truth используется метрика IoU (Intersection over Union) с порогом 0.5. Детекция считается правильной (TP), если:
1. IoU с ground truth bbox >= 0.5
2. Класс детекции совпадает с классом в ground truth
3. Ground truth bbox еще не был сопоставлен с другой детекцией

## Формат аннотаций

Файл аннотаций должен иметь следующий формат:
```
frame_number CLASS x1 y1 x2 y2
```

Пример:
```
0 CAR 339 82 446 169
1 BUS 100 50 200 150
2 TRUCK 300 200 450 350
```

Где:
- `frame_number` - номер кадра (начинается с 0)
- `CLASS` - название класса объекта (CAR, BUS, TRUCK, MOTORCYCLE, BICYCLE и т.д.)
- `x1, y1, x2, y2` - координаты ограничивающего прямоугольника в пикселях


## Результаты работы

После выполнения приложение выводит:
- Средние значения TPR и FDR по всем обработанным кадрам
- Качество детектирования (Quality Score) на основе TPR
- Сохраняет сводку результатов в файл `results_{args.model}_conf{args.conf_threshold}.txt`

При использовании опции `--save_output` обработанные изображения с нарисованными детекциями сохраняются в указанную директорию. На каждом изображении отображаются:
- Прямоугольники разных цветов для разных классов объектов
- Название класса и уверенность детекции в левом верхнем углу каждого прямоугольника
- Метрики TPR и FDR для текущего кадра

## Цветовая схема классов

- **car** (автомобиль) - Зеленый
- **bus** (автобус) - Синий
- **truck** (грузовик) - Красный
- **motorcycle** (мотоцикл) - Голубой
- **bicycle** (велосипед) - Пурпурный
- **train** (поезд) - Желтый
- **boat** (лодка) - Фиолетовый
- **airplane** (самолет) - Оранжевый


